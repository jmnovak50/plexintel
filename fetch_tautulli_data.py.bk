from dotenv import load_dotenv
import os
import time
import requests
import json
import psycopg2
import argparse
import pandas as pd
from datetime import datetime
import psycopg2
from pgvector.psycopg2 import register_vector
from pgvector import Vector
# Ollama embedding client (NAS-hosted EmbeddingGemma)
from ollama_embeddings import embed_texts
# Optional: batch sizing from env
EMBED_BATCH_SIZE = os.getenv("EMBED_BATCH_SIZE")

# âœ… Load environment variables
load_dotenv()

DB_CONFIG = {
    "dbname": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT")
}

TAUTULLI_API_URL = os.getenv("TAUTULLI_API_URL")
TAUTULLI_API_KEY = os.getenv("TAUTULLI_API_KEY")

def safe_int(value):
    """Convert value to an integer safely, returning None if conversion fails."""
    if value in [None, "", " ", "None"]:  # Handle empty strings, spaces, and None
        return None
    try:
        return int(value)
    except ValueError:
        return None


# âœ… Fetch data from Tautulli API
import requests
import json
import psycopg2
import os

def clear_tautulli_cache():
    """
    Clear Tautulli's metadata cache so new/changed items show up.
    Uses the same TAUTULLI_URL and TAUTULLI_API_KEY as the rest of the script.
    """
    params = {
        "apikey": TAUTULLI_API_KEY,
        "cmd": "delete_cache",
    }
    try:
        resp = requests.get(f"{TAUTULLI_URL}/api/v2", params=params, timeout=30)
        resp.raise_for_status()
        print("âœ… Cleared Tautulli cache via delete_cache")
    except Exception as e:
        print(f"âŒ ERROR clearing Tautulli cache: {e}")

# âœ… Fetch data from Tautulli API
def fetch_tautulli_data(endpoint, params=None):
    if params is None:
        params = {}
    params["cmd"] = endpoint  
    params["apikey"] = os.getenv("TAUTULLI_API_KEY")

    try:
        response = requests.get(os.getenv("TAUTULLI_API_URL"), params=params)
        # print(f"ğŸ” DEBUG: API call to {endpoint} returned status {response.status_code}")

        try:
            data = response.json()
            # print(f"ğŸ” DEBUG: Raw API Response: {json.dumps(data, indent=2)}")
            return data.get("response", {}).get("data", {})
        except json.JSONDecodeError:
            # print(f"âŒ ERROR: Invalid JSON response: {response.text}")
            return {}
    except requests.exceptions.RequestException as e:
        print(f"âŒ ERROR: Network error occurred: {e}")
        return {}

# âœ… Connect to PostgreSQL Database
def connect_to_db():
    try:
        conn = psycopg2.connect(
            dbname=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD"),
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT"),
        )
        # âœ… Tell psycopg2 how to handle pgvector's `vector` type
        register_vector(conn)

        cursor = conn.cursor()
        print("âœ… Connected to PostgreSQL database")
        return conn, cursor
    except Exception as e:
        print(f"âŒ ERROR: Could not connect to PostgreSQL: {e}")
        return None, None

    
def _media_text_for_embedding(row):
    """
    row = (rating_key, title, summary)
    Build a plain-text description for the embedding model.
    """
    rk, title, summary = row
    title = title or ""
    summary = summary or ""
    return f"{title} â€” {summary}".strip()

def generate_media_embeddings(batch_size: int = EMBED_BATCH_SIZE):
    """
    Generate embeddings for media items in `library` that are missing in `media_embeddings`.

    Assumes:
      - media_embeddings.embedding is vectors.vector(768) (VectorChord)
      - Ollama (embeddinggemma) is reachable via embed_texts()
    """
    print("ğŸ§  Generating media embeddings...")

    conn, cur = connect_to_db()
    if not conn:
        print("âŒ ERROR: Could not connect to database.")
        return

    print("âœ… Connected to PostgreSQL database")

    # Find library entries without embeddings
    cur.execute(
        """
        SELECT l.rating_key, l.title, COALESCE(l.summary, '')
        FROM library l
        LEFT JOIN media_embeddings me ON me.rating_key = l.rating_key
        WHERE me.rating_key IS NULL
        ORDER BY l.rating_key
        """
    )
    rows = cur.fetchall()
    if not rows:
        print("âœ… No media rows missing embeddings.")
        conn.close()
        return

    print(f"ğŸ“¦ Media items to embed: {len(rows)}")

    texts = [_media_text_for_embedding(r) for r in rows]
    vectors = embed_texts(texts, batch_size=batch_size)
    assert len(vectors) == len(rows)

    # Insert in batches to keep transactions reasonable
    for i in range(0, len(rows), batch_size):
        part_rows = rows[i:i + batch_size]
        part_vecs = vectors[i:i + batch_size]
        with conn:
            with conn.cursor() as cur2:
                for (rk, _t, _s), vec in zip(part_rows, part_vecs):
                    cur2.execute(
                        """
                        INSERT INTO media_embeddings (rating_key, embedding)
                        VALUES (%s, %s::real[]::vectors.vector)
                        ON CONFLICT (rating_key) DO NOTHING
                        """,
                        (rk, vec),
                    )

    conn.close()
    print("âœ… Media embeddings complete.")


# âœ… Convert Unix timestamp to datetime
from datetime import datetime, UTC

def convert_timestamp(timestamp):
    try:
        return datetime.fromtimestamp(int(timestamp), UTC).strftime('%Y-%m-%d %H:%M:%S') if timestamp else None
    except (ValueError, TypeError):
        return None

# âœ… Convert value to integer
def convert_to_int(value):
    try:
        return int(value) if value not in (None, "") else None
    except ValueError:
        return None

# âœ… Fetch all paginated results from API
def fetch_paginated_data(endpoint):
    all_results = []
    start = 0
    count = 100  # Adjust if needed
    while True:
        params = {"start": start, "length": count}
        data = fetch_tautulli_data(endpoint, params)
        if not data or "recordsFiltered" in data and len(data.get("data", [])) == 0:
            break
        all_results.extend(data.get("data", []))
        start += count
    return all_results

import requests

def get_library_media_info(section_id, limit=None):
    """
    Fetches basic metadata for movies and TV shows from the Plex library.

    Args:
        section_id (int): The library section ID (1 for Movies, 2 for TV Shows)

    Returns:
        list: A list of media items with basic details (rating_key, title, year, added_at)
    """
    all_results = []
    start = 0
    page_size = 100  # Adjust as needed for performance

    while True:
        print(f"ğŸ”„ DEBUG: Fetching media info (section_id={section_id}, start={start})")
        params = {
            "section_id": section_id,
            "start": start,
            "length": page_size
        }
        data = fetch_tautulli_data("get_library_media_info", params)

        if not data or "data" not in data:
            print(f"âš ï¸ WARNING: No data returned for section_id={section_id}, start={start}")
            break

        media_items = data["data"]
        print(f"ğŸ” DEBUG: Retrieved {len(media_items)} items")

        if not media_items:
            print(f"âš ï¸ DEBUG: Empty media_items for section_id={section_id}, stopping.")
            break
        all_results.extend(media_items)

        # Pagination check: If results are less than page size, we are at the last page
        if limit and len(all_results) >= limit:
            all_results = all_results[:limit]
            break

        if len(media_items) < page_size:
            break

        start += page_size  # Move to next page

    print(f"âœ… DEBUG: Fetched total {len(all_results)} items for section_id={section_id}")
    return all_results

def get_children_metadata(parent_rating_key):
    all_children = []
    start = 0
    page_size = 50  

    while True:
        print(f"ğŸ”„ Fetching children metadata for parent_rating_key={parent_rating_key}, start={start}")
        params = {
            "rating_key": parent_rating_key,
            "start": start,
            "length": page_size
        }
        data = fetch_tautulli_data("get_children_metadata", params)  

        # âœ… Ensure we're extracting from the correct key
        if not data or "children_list" not in data:
            print(f"âš ï¸ WARNING: No children found for parent_rating_key={parent_rating_key}")
            break

        children_list = data["children_list"]  # Extracting directly from children_list
        if not children_list:
            print(f"âš ï¸ No valid children data for parent_rating_key={parent_rating_key}")
            break

        print(f"ğŸ“¦ Batch received: {len(children_list)} items for parent_rating_key={parent_rating_key}")  # <-- Add this

        all_children.extend(children_list)

        if len(children_list) < page_size:
            break

        start += page_size  

    print(f"âœ… Retrieved {len(all_children)} children for parent_rating_key={parent_rating_key}")
    return all_children



#def should_revise_metadata(metadata):
#    """
#    Determines if metadata requires revision.
#    Returns True if revision needed, otherwise False.
#    """
#    # Example condition: revise if crucial fields are missing or incomplete
#    required_fields = ['title', 'media_type', 'year']
#    return any(not metadata.get(field) for field in required_fields)


#def revise_metadata(metadata):
#    """
#    Perform necessary revisions to metadata.
#    Returns the revised metadata.
#    """
#    # Example revision logic
#    revised_metadata = metadata.copy()
#
#    if not revised_metadata.get('title'):
#        revised_metadata['title'] = "Unknown Title"
#
#    if not revised_metadata.get('media_type'):
#       revised_metadata['media_type'] = "movie"  # Default to 'movie' or other type
#
#    if not revised_metadata.get('year'):
#        revised_metadata['year'] = 0  # Default or placeholder year
#
#    print(f"ğŸ› ï¸ Revised metadata applied for rating_key={metadata.get('rating_key')}")
#    return revised_metadata


def get_metadata(rating_key):
    print(f"ğŸ”„ Fetching detailed metadata for rating_key={rating_key}...")
    params = {"rating_key": rating_key}
    metadata = fetch_tautulli_data("get_metadata", params)  # Fetch data directly

    if not metadata:
        print(f"âš ï¸ WARNING: No metadata found for rating_key={rating_key}. Trying cache clear...")
        clear_tautulli_cache()
        time.sleep(1)  # Wait for cache to clear
        metadata = fetch_tautulli_data("get_metadata", params)  # Retry fetching metadata
        if not metadata:
            print(f"âŒ ERROR: Still no metadata found for rating_key={rating_key} after cache clear.")
            return None

    print(f"âœ… SUCCESS: Metadata retrieved for rating_key={rating_key}: {metadata}")
    print(metadata)  # ğŸ” Print the full metadata response to debug

    media_type = metadata.get("media_type")

    # âœ… Debug Print for season and episode
    print(f"ğŸ” DEBUG: media_type={media_type}, parent_media_index={metadata.get('parent_media_index')}, media_index={metadata.get('media_index')}")


    # âœ… Ensure correct extraction of season_number & episode_number for TV episodes
    season_number = int(metadata["parent_media_index"]) if media_type == "episode" and metadata.get("parent_media_index") else None
    episode_number = int(metadata["media_index"]) if media_type == "episode" and metadata.get("media_index") else None

    # **ğŸ” Debug print before returning**
    print(f"ğŸ” DEBUG: Extracted metadata for rating_key={rating_key}")
    print(f"   - media_type: {media_type}")
    print(f"   - season_number: {season_number}")
    print(f"   - episode_number: {episode_number}")

    return {
        "rating_key": metadata.get("rating_key"),
        "title": metadata.get("title"),
        "summary": metadata.get("summary"),
        "rating": metadata.get("rating"),
        "added_at": metadata.get("added_at"),
        "year": int(metadata["year"]) if metadata.get("year") else None,
        "duration": int(metadata["duration"]) if metadata.get("duration") else None,
        "media_type": media_type,
        "parent_rating_key": metadata.get("parent_rating_key"),
        "grandparent_title": metadata.get("grandparent_title"),  # TV Show name
        "parent_title": metadata.get("parent_title"),  # Season name
        "season_number": season_number,  # âœ… FIXED: Season number for TV episodes
        "episode_number": episode_number,  # âœ… FIXED: Episode number for TV episodes
        "genres": metadata.get("genres", []),  # âœ… Extract list of genres
        "actors": metadata.get("actors", []),  # âœ… Extract list of actors
        "directors": metadata.get("directors", []),  # âœ… Extract list of directors
    }

# âœ… Fetch library metadata
def get_library_data(movies_limit=None, tv_limit=None):
    library_data = []

    # Fetch Movies (simple)
    movies = get_library_media_info(section_id=1, limit=movies_limit)
    print(f"ğŸš¨ DEBUG Movies fetched: {len(movies)}")  # Debug after movies fetched

    for movie in movies:
        metadata = get_metadata(movie["rating_key"])
        if metadata:
            library_data.append(metadata)
        else:
            print(f"âš ï¸ Movie metadata missing: rating_key={movie['rating_key']}")

    # Fetch TV Shows (complex hierarchy)
    tv_shows = get_library_media_info(section_id=2, limit=tv_limit)
    print(f"ğŸš¨ DEBUG TV Shows fetched: {len(tv_shows)}")  # Debug after TV Shows fetched

    for show in tv_shows:
        print(f"ğŸ” Fetching seasons for show: {show['title']} ({show['rating_key']})")
        seasons = get_children_metadata(show["rating_key"])  # âœ… Get Seasons
        seasons = [s for s in seasons if s.get("media_type") == "season"]  # âœ… Ensure only seasons
        print(f"ğŸš¨ DEBUG Seasons fetched for show {show['rating_key']}: {len(seasons)}")

        for season in seasons:
            print(f"ğŸ” Fetching episodes for season: {season['title']} ({season['rating_key']})")
            episodes = get_children_metadata(season["rating_key"])  # âœ… Get Episodes
            episodes = [e for e in episodes if e.get("media_type") == "episode"]  # âœ… Ensure only episodes
            print(f"ğŸš¨ DEBUG Episodes fetched for season {season['rating_key']}: {len(episodes)}")

            for episode in episodes:
                episode_metadata = get_metadata(episode["rating_key"])
                if episode_metadata:
                    library_data.append(episode_metadata)  # âœ… Store Episodes
                else:
                    print(f"âš ï¸ Episode metadata missing: rating_key={episode['rating_key']}")

    return library_data

# âœ… Fetch watch history with pagination
def get_watch_history():
    return fetch_paginated_data("get_history")

# âœ… Store library metadata in PostgreSQL
def store_library_data(conn, cursor, library_data):
    print("ğŸ“Œ Full library_data before inserting:", library_data)

    if not library_data:
        print("âš ï¸ No library data found. Skipping library insert.")
        return

    insert_library = """
        INSERT INTO library (
            rating_key, title, year, duration, media_type, summary, rating, added_at, 
            season_number, episode_number, parent_rating_key, show_title, episode_title, episode_summary
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, TO_TIMESTAMP(%s), %s, %s, %s, %s, %s, %s
        )
        ON CONFLICT (rating_key) DO UPDATE SET
            title = EXCLUDED.title,
            year = EXCLUDED.year,
            duration = EXCLUDED.duration,
            media_type = EXCLUDED.media_type,
            summary = EXCLUDED.summary,
            rating = EXCLUDED.rating,
            added_at = EXCLUDED.added_at,
            season_number = EXCLUDED.season_number,
            episode_number = EXCLUDED.episode_number,
            parent_rating_key = EXCLUDED.parent_rating_key,
            show_title = EXCLUDED.show_title,
            episode_title = EXCLUDED.episode_title,
            episode_summary = EXCLUDED.episode_summary;
    """

    successful_inserts = 0

    for item in library_data:
        # âœ… Ensure `rating_key` is assigned at the very beginning of the loop
        rating_key = item.get("rating_key")
        if not rating_key:
            print(f"âš ï¸ Skipped item missing rating_key: {item}")
            continue  # Prevents using `rating_key` before it's assigned

        # âœ… Extract values safely
        title = item.get("title")
        year = safe_int(item.get("year"))
        duration = safe_int(item.get("duration"))
        media_type = item.get("media_type")
        summary = item.get("summary")
        rating = item.get("rating")
        added_at = safe_int(item.get("added_at"))
        season_number = safe_int(item.get("season_number"))
        episode_number = safe_int(item.get("episode_number"))
        parent_rating_key = safe_int(item.get("parent_rating_key"))
        show_title = item.get("grandparent_title") if media_type == 'episode' else item.get("parent_title")
        episode_title = item.get("title") if media_type == 'episode' else None
        episode_summary = summary if media_type == 'episode' else None

        # âœ… Print debug info AFTER values are assigned
        print(f"ğŸ” DEBUG: Checking extracted values for rating_key={rating_key}")
        print(f"   - season_number: {season_number}")
        print(f"   - episode_number: {episode_number}")

        # Extract lists for relationships
        genres = item.get("genres", [])
        actors = item.get("actors", [])
        directors = item.get("directors", [])

        # âœ… Now safe to print debug info (all variables exist)
        print("ğŸš¨ About to INSERT explicitly:", {
            "rating_key": rating_key,
            "title": title,
            "year": year,
            "duration": duration,
            "media_type": media_type,
            "summary": summary[:30] if summary else None,
            "rating": rating,
            "added_at": added_at,
            "season_number": season_number,
            "episode_number": episode_number,
            "parent_rating_key": parent_rating_key,
            "show_title": show_title,
            "episode_title": episode_title,
            "episode_summary": episode_summary[:30] if episode_summary else None,
            "genres": genres,
            "actors": actors,
            "directors": directors
        })

        try:
            cursor.execute(insert_library, (
                rating_key, title, year, duration, media_type, summary, rating,
                added_at, season_number, episode_number, parent_rating_key,
                show_title, episode_title, episode_summary
            ))

            # âœ… Store associated genres, actors, and directors (only if they exist)
            if genres:
                store_genres(conn, cursor, rating_key, genres)
            if actors:
                store_actors(conn, cursor, rating_key, actors)
            if directors:
                store_directors(conn, cursor, rating_key, directors)

            print(f"âœ… Successfully inserted: {rating_key}")
            successful_inserts += 1
        except Exception as e:
            print(f"âš ï¸ EXCEPTION inserting rating_key={rating_key}: {e}")
            conn.rollback()
            
    print("ğŸ” Checking database commitâ€¦")
    conn.commit()
    print(f"âœ… Stored {successful_inserts}/{len(library_data)} library items into database.")



def store_genres(conn, cursor, rating_key, genres):
    if not genres:
        return

    insert_genre = """
        INSERT INTO genres (name) 
        VALUES (%s) 
        ON CONFLICT (name) DO NOTHING;
    """
    insert_media_genre = """
        INSERT INTO media_genres (media_id, genre_id) 
        SELECT %s, id FROM genres WHERE name = %s
        ON CONFLICT DO NOTHING;
    """

    for genre in genres:
        try:
            cursor.execute(insert_genre, (genre,))  # Ensure genre exists
            cursor.execute(insert_media_genre, (rating_key, genre))  # Link to media
        except Exception as e:
            print(f"âš ï¸ ERROR inserting genre '{genre}' for rating_key={rating_key}: {e}")
            conn.rollback()

    conn.commit()



def store_actors(conn, cursor, rating_key, actors):
    if not actors:
        return

    insert_actor = """
        INSERT INTO actors (name) 
        VALUES (%s) 
        ON CONFLICT (name) DO NOTHING;
    """
    insert_media_actor = """
        INSERT INTO media_actors (media_id, actor_id) 
        SELECT %s, id FROM actors WHERE name = %s
        ON CONFLICT DO NOTHING;
    """

    for actor in actors:
        try:
            cursor.execute(insert_actor, (actor,))  # Ensure actor exists
            cursor.execute(insert_media_actor, (rating_key, actor))  # Link to media
        except Exception as e:
            print(f"âš ï¸ ERROR inserting actor '{actor}' for rating_key={rating_key}: {e}")
            conn.rollback()

    conn.commit()



def store_directors(conn, cursor, rating_key, directors):
    if not directors:
        return

    insert_director = """
        INSERT INTO directors (name) 
        VALUES (%s) 
        ON CONFLICT (name) DO NOTHING;
    """
    insert_media_director = """
        INSERT INTO media_directors (media_id, director_id) 
        SELECT %s, id FROM directors WHERE name = %s
        ON CONFLICT DO NOTHING;
    """

    for director in directors:
        try:
            cursor.execute(insert_director, (director,))  # Ensure director exists
            cursor.execute(insert_media_director, (rating_key, director))  # Link to media
        except Exception as e:
            print(f"âš ï¸ ERROR inserting director '{director}' for rating_key={rating_key}: {e}")
            conn.rollback()

    conn.commit()




# âœ… Store watch history in PostgreSQL
def store_watch_history(conn, cursor, watch_history):           
    insert_watch = """
        INSERT INTO watch_history (rating_key, watched_at, played_duration, percent_complete, watch_id, media_type, username, title, episode_title, season_number, episode_number, friendly_name)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (watch_id) 
        DO UPDATE SET
                played_duration = EXCLUDED.played_duration, 
                percent_complete = EXCLUDED.percent_complete,
                watched_at = EXCLUDED.watched_at
"""
    for record in watch_history:
        watch_id = record.get("watch_id") or record.get("id")
        rating_key = record.get("rating_key")
        if watch_id is None or rating_key is None:
            print(f"âš ï¸ Skipping watch history insert due to missing watch_id or rating_key: {record}")
            continue
        try:
            cursor.execute(insert_watch, (
                rating_key, convert_timestamp(record.get("date")), record.get("play_duration"), 
                record.get("percent_complete"), watch_id, record.get("media_type"),
                record.get("user"), record.get("grandparent_title") if record.get("media_type") == "episode" else record.get("title"),
                record.get("title") if record.get("media_type") == "episode" else None,
                convert_to_int(record.get("parent_media_index")), convert_to_int(record.get("media_index")),
                record.get("friendly_name")  # Added friendly_name here
            ))
        except Exception as e:
            print(f"âŒ Error inserting watch history for watch_id={watch_id}: {e}")
            conn.rollback()

#def fetch_existing_library_data(cursor):
#    query = "SELECT rating_key, title FROM library WHERE media_type = 'show'"
#    cursor.execute(query)
#    
#    stored_shows = cursor.fetchall()  # Fetch list of tuples
#
#    # âœ… Convert list of tuples into a list of dictionaries
#    return [{"rating_key": row[0], "title": row[1]} for row in stored_shows]



#def get_incremental_library_data(cursor):
#    print("ğŸ”„ Checking for new or updated library items...")
#
#    # âœ… Query existing data from PostgreSQL
#    cursor.execute("SELECT rating_key, added_at FROM library")
#    existing_library = {row[0]: row[1] for row in cursor.fetchall()}  # Dictionary {rating_key: updated_at}
#
#    # âœ… Fetch latest library info from Tautulli
#    latest_library = get_library_media_info(section_id=1) + get_library_media_info(section_id=2)
#
#    # âœ… Identify new or updated items
#    to_process = []
#    for item in latest_library:
#        rating_key = item["rating_key"]
#        updated_at = int(item.get("updated_at", 0)) if item.get("updated_at") else 0  # Ensure int type
#
#        if rating_key not in existing_library:
#            print(f"ğŸ†• New item: {item.get('title', rating_key)}")
#            to_process.append(item)
#        elif updated_at > (existing_library.get(rating_key) or 0):
#            print(f"ğŸ”„ Updated item: {item.get('title', rating_key)}")
#            to_process.append(item)
#
#    print(f"âœ… Found {len(to_process)} new or updated media items.")
#    return to_process

def run_incremental_load():
    print("ğŸ” Running incremental update...")

    conn, cursor = connect_to_db()
    if not conn:
        print("âŒ ERROR: Could not connect to database.")
        return

    # 1. Fetch all current rating_keys from DB
    cursor.execute("SELECT rating_key FROM library")
    existing_keys = {str(row[0]) for row in cursor.fetchall()}

    # 2. Get movies
    print("ğŸï¸ Fetching movies...")
    movies = get_library_media_info(section_id=1)
    new_items = [m for m in movies if str(m["rating_key"]) not in existing_keys]

    # 3. Get episodes via show â†’ season â†’ episode
    print("ğŸ“º Fetching episodes...")
    shows = get_library_media_info(section_id=2)
    for show in shows:
        show_key = show["rating_key"]
        seasons = get_children_metadata(show_key)
        for season in seasons:
            if season.get("media_type") != "season":
                continue
            season_key = season["rating_key"]
            episodes = get_children_metadata(season_key)
            for ep in episodes:
                if ep.get("media_type") == "episode":
                    rk = str(ep["rating_key"])
                    if rk not in existing_keys:
                        new_items.append(ep)

    print(f"ğŸ“Œ New media items to insert: {len(new_items)}")

    # 4. Fetch detailed metadata for each new item
    enriched = []
    for item in new_items:
        metadata = get_metadata(item["rating_key"])
        if metadata:
            enriched.append(metadata)
        else:
            print(f"âš ï¸ Could not fetch metadata for rating_key={item['rating_key']}")

    if enriched:
        print(f"ğŸ“¥ Inserting {len(enriched)} new media entries...")
        store_library_data(conn, cursor, enriched)
    else:
        print("âœ… No new content to add.")

    # 5. Sync new watch history
    sync_new_watch_history(conn, cursor)

    # 6. Optionally embed new stuff immediately after sync
    if os.getenv("ENABLE_EMBED_MEDIA", "1") == "1":
        generate_media_embeddings()

    if os.getenv("ENABLE_EMBED_WATCHES", "1") == "1":
        generate_watch_embeddings()

    conn.commit()
    conn.close()
    print("âœ… Incremental update complete.")


def recover_missing_media(dry_run=False):
    print("ğŸ” Starting comprehensive media recovery process...")
    conn, cursor = connect_to_db()
    if not conn:
        print("âŒ ERROR: Could not connect to database.")
        return

    all_rating_keys = set()

    # 1. Get all movie rating_keys
    print("ğŸï¸ Collecting movies...")
    movies = get_library_media_info(section_id=1)
    all_rating_keys.update(str(item["rating_key"]) for item in movies)

    # 2. Get all episode rating_keys from shows
    print("ğŸ“º Collecting episodes from all shows...")
    shows = get_library_media_info(section_id=2)
    for show in shows:
        show_key = show["rating_key"]
        seasons = get_children_metadata(show_key)
        for season in seasons:
            if season.get("media_type") != "season":
                continue
            season_key = season["rating_key"]
            episodes = get_children_metadata(season_key)
            for ep in episodes:
                if ep.get("media_type") == "episode":
                    all_rating_keys.add(str(ep["rating_key"]))

    print(f"ğŸ“¦ Total media rating_keys found in Plex: {len(all_rating_keys)}")

    # 3. Get existing rating_keys from DB
    cursor.execute("SELECT rating_key FROM library")
    existing_keys = {str(row[0]) for row in cursor.fetchall()}

    # 4. Diff
    missing_keys = all_rating_keys - existing_keys
    print(f"ğŸ§© Missing from DB: {len(missing_keys)}")

    # 5. Fetch metadata and recover
    recovered_items = []
    for rk in missing_keys:
        print(f"ğŸ” Attempting fetch for rating_key={rk}")
        metadata = get_metadata(rk)
        if metadata:
            print(f"[RECOVER] rating_key={metadata['rating_key']}, type={metadata.get('media_type')}, title={metadata.get('title')}")
            recovered_items.append(metadata)
        else:
            print(f"âš ï¸ Could not fetch metadata for rating_key={rk}")

    # 6. Insert or preview
    if recovered_items:
        if dry_run:
            print(f"\nğŸ“‹ DRY RUN: Would recover {len(recovered_items)} items:")
            for item in recovered_items:
                print(f" - {item['media_type']} | {item['title']} (rating_key={item['rating_key']})")
        else:
            print(f"\nğŸ“¥ Inserting {len(recovered_items)} recovered items into the database...")
            store_library_data(conn, cursor, recovered_items)
    else:
        print("âœ… No recoverable items found.")

    conn.close()
    print("ğŸ”’ Database connection closed after recovery.")

#def sync_new_library_items(conn, cursor, dry_run=False):
#    print("ğŸ”„ Starting library sync for new media...")
#
#    all_rating_keys = set()
#
#    # 1. Fetch all movie rating_keys
#    print("ğŸï¸ Fetching movies...")
#    movies = get_library_media_info(section_id=1)
#    all_rating_keys.update(str(item["rating_key"]) for item in movies)
#
#    # 2. Fetch all episode rating_keys
#    print("ğŸ“º Fetching episodes from shows...")
#    shows = get_library_media_info(section_id=2)
#    for show in shows:
#        show_key = show["rating_key"]
#        seasons = get_children_metadata(show_key)
#        for season in seasons:
#            if season.get("media_type") != "season":
#                continue
#            season_key = season["rating_key"]
#            episodes = get_children_metadata(season_key)
#            for ep in episodes:
#                if ep.get("media_type") == "episode":
#                    all_rating_keys.add(str(ep["rating_key"]))
#
#    print(f"ğŸ“¦ Total rating_keys from Plex: {len(all_rating_keys)}")
#
#    # 3. Get existing rating_keys from DB
#    cursor.execute("SELECT rating_key FROM library")
#    existing_keys = {str(row[0]) for row in cursor.fetchall()}
#
#    missing_keys = all_rating_keys - existing_keys
#    print(f"ğŸ§© New media not in DB: {len(missing_keys)}")
#
#    # 4. Fetch and insert metadata for missing items
#    recovered_items = []
#    for rk in missing_keys:
#        print(f"ğŸ” Attempting fetch for rating_key={rk}")
#        metadata = get_metadata(rk)
#        if metadata:
#            recovered_items.append(metadata)
#        else:
#            print(f"âš ï¸ Could not fetch metadata for rating_key={rk}")
#
#    if recovered_items:
#        if dry_run:
#            print(f"\nğŸ“‹ DRY RUN: Would insert {len(recovered_items)} items:")
#            for item in recovered_items:
#                print(f" - {item['media_type']} | {item['title']} (rating_key={item['rating_key']})")
#        else:
#            print(f"ğŸ“¥ Inserting {len(recovered_items)} items into library table...")
#            store_library_data(conn, cursor, recovered_items)
#    else:
#        print("âœ… No new media to insert.")

from datetime import datetime, timezone
import json

def fetch_all_watch_history(after_ts):
    print(f"ğŸ•µï¸ Fetching all Tautulli history after {after_ts} (paginated)")
    all_rows = []
    start = 0
    page_size = 1000

    while True:
        response = fetch_tautulli_data("get_history", {
            "after": after_ts,
            "order_dir": "asc",
            "length": page_size,
            "start": start
        })
        rows = response.get("data", [])
        print(f"ğŸ“¦ Page {start // page_size + 1}: Retrieved {len(rows)} rows")

        if not rows:
            break

        all_rows.extend(rows)
        if len(rows) < page_size:
            break

        start += page_size

    return all_rows

def sync_new_watch_history(conn, cursor):
    print("ğŸ¬ Syncing new watch history...")

    cursor.execute("SELECT MAX(watched_at) FROM watch_history")
    last_seen = cursor.fetchone()[0]

    # Convert to UNIX seconds (Tautulli expects this)
    if last_seen is None:
        after_ts = 0
    else:
        if last_seen.tzinfo is None:
            last_seen = last_seen.replace(tzinfo=timezone.utc)
        after_ts = int(last_seen.timestamp()) - 60

    rows = fetch_all_watch_history(after_ts)
    print(f"ğŸ§ª Tautulli returned {len(rows)} raw rows total")

    if not rows:
        print("âœ… No new watch history found.")
        return

    valid_rows = []
    skipped_rows = []

    for r in rows:
        if r.get("id") and r.get("rating_key"):
            r["watch_id"] = r["id"]  # normalize for DB
            valid_rows.append(r)
        else:
            skipped_rows.append(r)

    print(f"ğŸ“¥ Found {len(valid_rows)} valid watch history rows. Inserting...")

    if skipped_rows:
        print(f"âš ï¸ Skipped {len(skipped_rows)} rows due to missing id or rating_key")
        print("âš ï¸ Example skipped row:")
        print(json.dumps(skipped_rows[0], indent=2))

    if valid_rows:
        latest_insert = max([r["date"] for r in valid_rows])
        print(f"ğŸ†• Most recent fetched watched_at: {datetime.utcfromtimestamp(latest_insert)} UTC")

    store_watch_history(conn, cursor, valid_rows)

def generate_media_embeddings(batch_size: int = EMBED_BATCH_SIZE):
    """
    Generate embeddings for media items in `library` that are missing in `media_embeddings`.

    Assumes:
      - media_embeddings.embedding is pgvector `vector(768)`
      - Ollama (embeddinggemma) is reachable via embed_texts()
    """
    print("ğŸ§  Generating media embeddings...")

    conn, cur = connect_to_db()
    if not conn:
        print("âŒ ERROR: Could not connect to database.")
        return

    print("âœ… Connected to PostgreSQL database")

    # Find library entries without embeddings
    cur.execute(
        """
        SELECT l.rating_key, l.title, COALESCE(l.summary, '')
        FROM library l
        LEFT JOIN media_embeddings me ON me.rating_key = l.rating_key
        WHERE me.rating_key IS NULL
        ORDER BY l.rating_key
        """
    )
    rows = cur.fetchall()
    if not rows:
        print("âœ… No media rows missing embeddings.")
        conn.close()
        return

    print(f"ğŸ“¦ Media items to embed: {len(rows)}")

    texts = [_media_text_for_embedding(r) for r in rows]
    vectors = embed_texts(texts, batch_size=batch_size)
    assert len(vectors) == len(rows)

    raw_bs = batch_size
    try:
        norm_bs = int(str(raw_bs).strip().split()[0])
    except Exception:
        norm_bs = 128  # safe fallback

    batch_size = norm_bs  # from here on, batch_size is a clean int

    # Insert in batches to keep transactions reasonable
    for i in range(0, len(rows), batch_size):
        part_rows = rows[i:i + batch_size]
        part_vecs = vectors[i:i + batch_size]
        with conn:
            with conn.cursor() as cur2:
                for (rk, _t, _s), vec in zip(part_rows, part_vecs):
                    cur2.execute(
                        """
                        INSERT INTO media_embeddings (rating_key, embedding)
                        VALUES (%s, %s)
                        ON CONFLICT (rating_key) DO NOTHING
                        """,
                        (rk, Vector(vec)),
                    )

    conn.close()
    print("âœ… Media embeddings complete.")

def _watch_text_for_embedding(
    title, username, percent, played_duration,
    season_number, episode_number,
    genres, actors, directors
):
    season_str = f"S{season_number}" if season_number is not None else ""
    episode_str = f"E{episode_number}" if episode_number is not None else ""

    watch_str = f"watched {percent}%" if percent is not None else "watched"
    if played_duration is not None:
        minutes_played = played_duration // 60
        watch_str += f" for {minutes_played} minutes"

    parts = [
        f"{title} {season_str}{episode_str} by {username}, {watch_str}",
        f"Genres: {genres}" if genres else "",
        f"Actors: {actors}" if actors else "",
        f"Directors: {directors}" if directors else "",
    ]
    return ". ".join(p for p in parts if p).strip()

def generate_watch_embeddings(batch_size: int = 16):
    """
    Generate embeddings for watch_history rows that are missing watch_embeddings.
    Uses library + genre/actor/director context to build the text.
    Assumes watch_embeddings.embedding is vectors.vector(768).
    """
    print("ğŸ§  Generating watch embeddings via Ollama (EmbeddingGemma)...")

    conn, cur = connect_to_db()
    if not conn:
        print("âŒ ERROR: Could not connect to database.")
        return

    # Pull only watch rows that don't yet have an embedding
    cur.execute(
        """
        SELECT
            wh.watch_id,
            l.title,
            wh.username,
            wh.percent_complete,
            wh.played_duration,
            wh.season_number,
            wh.episode_number,
            (
                SELECT string_agg(g.name, ', ')
                FROM media_genres mg
                JOIN genres g ON mg.genre_id = g.id
                WHERE mg.media_id = l.rating_key
            ) AS genres,
            (
                SELECT string_agg(a.name, ', ')
                FROM media_actors ma
                JOIN actors a ON ma.actor_id = a.id
                WHERE ma.media_id = l.rating_key
            ) AS actors,
            (
                SELECT string_agg(d.name, ', ')
                FROM media_directors md
                JOIN directors d ON md.director_id = d.id
                WHERE md.media_id = l.rating_key
            ) AS directors
        FROM watch_history wh
        JOIN library l ON wh.rating_key = l.rating_key
        LEFT JOIN watch_embeddings we ON wh.watch_id::text = we.watch_id
        WHERE we.watch_id IS NULL
        """
    )
    rows = cur.fetchall()

    if not rows:
        print("âœ… No watch rows missing embeddings.")
        conn.close()
        return

    print(f"ğŸ§  Generating embeddings for {len(rows)} watch events...")

    texts = [
        _watch_text_for_embedding(
            title, username, percent, played_duration,
            season_number, episode_number,
            genres, actors, directors,
        )
        for (
            watch_id, title, username, percent, played_duration,
            season_number, episode_number, genres, actors, directors
        ) in rows
    ]

    vectors = embed_texts(texts, batch_size=batch_size)
    assert len(vectors) == len(rows)

    for i in range(0, len(rows), batch_size):
        part_rows = rows[i:i + batch_size]
        part_vecs = vectors[i:i + batch_size]
        with conn:
            with conn.cursor() as cur2:
                for (
                    (watch_id, _title, _username, _percent, _pd,
                     _sn, _en, _g, _a, _d),
                    vec,
                ) in zip(part_rows, part_vecs):
                    cur2.execute(
                        """
                        INSERT INTO watch_embeddings (watch_id, embedding)
                        VALUES (%s, %s)
                        ON CONFLICT (watch_id) DO NOTHING
                        """,
                        (watch_id, Vector(vec)),
                    )

    conn.close()
    print("âœ… Watch embeddings complete.")

# âœ… Main execution
def main():
    print("ğŸš€ Script started...")
    """
    Main function to fetch and store library data and watch history from Tautulli API to PostgreSQL.
    """
    print("ğŸš€ Script started...")
    try:
        # Establish database connection
        conn = psycopg2.connect(
            dbname=DB_CONFIG["dbname"],
            user=DB_CONFIG["user"],
            password=DB_CONFIG["password"],
            host=DB_CONFIG["host"],
            port=DB_CONFIG["port"]
        )
        with conn.cursor() as cursor:
            # Fetch and store library data
            library_data = get_library_data(movies_limit=None, tv_limit=None)
            print(f"ğŸš¨ DEBUG: Retrieved library data count: {len(library_data)}")
            if len(library_data) == 0:
                print("ğŸš¨ DEBUG: Library data is EMPTY after get_library_data()!")
            store_library_data(conn, cursor, library_data)
            

            # Fetch and store watch history
            watch_history = get_watch_history()
            store_watch_history(conn, cursor, watch_history)
        # ğŸ”¥ Crucial: commit database changes after cursor block
        conn.commit()
        print("âœ… Data committed successfully.")

    except Exception as e:
        print(f"âŒ ERROR: {e}")
        conn.rollback()
    finally:
        if conn:
            conn.close()
            print("ğŸ”’ Database connection closed.")

# To run media recovery from CLI
# recover_missing_media()
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Plex/Tautulli Media Metadata Importer")
    parser.add_argument(
        "--mode",
    choices=["full", "incremental", "recover", "embeddings", "watch_embeddings"],
    default="full",
    help="Select run mode: full (default), incremental, recover missing items, generate embeddings, or generate watch history embeddings"
)
parser.add_argument(
    "--dry-run",
    action="store_true",
    help="If set, missing items will only be printed and NOT inserted into the database"
)
args = parser.parse_args()


print(f"ğŸš€ Starting script in '{args.mode}' mode...")

if args.mode == "full":
    main()
elif args.mode == "incremental":
    run_incremental_load()
elif args.mode == "recover":
    recover_missing_media(dry_run=args.dry_run)
elif args.mode == "embeddings":
    # Media embeddings via Ollama + VectorChord
    generate_media_embeddings()
elif args.mode == "watch_embeddings":
    # Watch embeddings via Ollama + VectorChord
    generate_watch_embeddings()