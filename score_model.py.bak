from dotenv import load_dotenv
load_dotenv()

import os
import pandas as pd
import numpy as np
import joblib
import psycopg2
from sqlalchemy import create_engine
from sqlalchemy import text
import ast
from datetime import datetime

import xgboost as xgb
from sklearn.preprocessing import MultiLabelBinarizer

os.chdir(os.path.dirname(os.path.abspath(__file__)))

DB_URL = os.getenv("DATABASE_URL", "postgresql://jmnovak:brigid@localhost:5432/plexintel")

def get_engine():
    return create_engine(DB_URL)

def get_unwatched_media(username):
    engine = get_engine()
    query = """
        SELECT 
            m.rating_key,
            m.title,
            e.embedding,
            m.year,
            g.genre_tags,
            a.actor_tags,
            d.director_tags,
            ue.embedding AS user_embedding
        FROM library m
        JOIN media_embeddings e ON m.rating_key = e.rating_key
        JOIN user_embeddings ue ON ue.username = %s
        LEFT JOIN watch_history w ON m.rating_key = w.rating_key AND w.username = %s

        -- JOIN genre tags
        LEFT JOIN (
            SELECT mg.media_id, STRING_AGG(g.name, ',') AS genre_tags
            FROM media_genres mg
            JOIN genres g ON mg.genre_id = g.id
            GROUP BY mg.media_id
        ) g ON g.media_id = m.rating_key

        -- JOIN actor tags
        LEFT JOIN (
            SELECT ma.media_id, STRING_AGG(a.name, ',') AS actor_tags
            FROM media_actors ma
            JOIN actors a ON ma.actor_id = a.id
            GROUP BY ma.media_id
        ) a ON a.media_id = m.rating_key

        -- JOIN director tags
        LEFT JOIN (
            SELECT md.media_id, STRING_AGG(d.name, ',') AS director_tags
            FROM media_directors md
            JOIN directors d ON md.director_id = d.id
            GROUP BY md.media_id
        ) d ON d.media_id = m.rating_key

        WHERE w.rating_key IS NULL
    """
    df = pd.read_sql(query, engine, params=(username, username))
    return df

def preprocess_for_scoring(df, feature_names_template):
    def safe_embedding_parse(x):
        if isinstance(x, str):
            return np.array(ast.literal_eval(x), dtype=np.float32)
        return np.array(x, dtype=np.float32)

    def get_decade_flags(year):
        if year is None or pd.isna(year):
            return {}
        try:
            decade = int(year) // 10 * 10
            return {f"is_{decade}s": 1}
        except:
            return {}

    df['genres'] = df['genre_tags'].fillna('').apply(lambda x: x.split(',') if x else [])
    df['actors'] = df['actor_tags'].fillna('').apply(lambda x: x.split(',') if x else [])
    df['directors'] = df['director_tags'].fillna('').apply(lambda x: x.split(',') if x else [])
    df['media_embedding'] = df['embedding'].apply(safe_embedding_parse)
    df['user_embedding'] = df['user_embedding'].apply(safe_embedding_parse)
    df['embedding'] = df.apply(lambda row: np.concatenate([row['media_embedding'], row['user_embedding']]), axis=1)

    decade_df = df['year'].apply(get_decade_flags).apply(pd.Series).fillna(0).astype(int)

    genre_top = [f.split("_", 1)[1] for f in feature_names_template if f.startswith("genre_")]
    actor_top = [f.split("_", 1)[1] for f in feature_names_template if f.startswith("actor_")]
    director_top = [f.split("_", 1)[1] for f in feature_names_template if f.startswith("director_")]

    genre_bin = MultiLabelBinarizer(classes=genre_top)
    actor_bin = MultiLabelBinarizer(classes=actor_top)
    director_bin = MultiLabelBinarizer(classes=director_top)

    genre_features = genre_bin.fit_transform(df['genres'])
    actor_features = actor_bin.fit_transform(df['actors'])
    director_features = director_bin.fit_transform(df['directors'])

    X = np.vstack(df['embedding'].values)
    X = np.hstack((X, genre_features, actor_features, director_features, decade_df.values))
    print("üìÜ Decade columns added:", list(decade_df.columns))

    return X

def cosine_similarity_batch(user_embeddings, media_embeddings):
    dot_products = np.einsum('ij,ij->i', user_embeddings, media_embeddings)
    norms_user = np.linalg.norm(user_embeddings, axis=1)
    norms_media = np.linalg.norm(media_embeddings, axis=1)
    similarities = dot_products / (norms_user * norms_media)
    return similarities

def score_and_store(username):
    import shap
    print("üì• Loading model...")
    model = joblib.load("xgb_model.pkl")
    booster = model.get_booster()

    print(f"üìä Fetching unwatched media for {username}...")
    df = get_unwatched_media(username)
    if df.empty:
        print("‚úÖ No unwatched items to score.")
        return

    print("üßπ Preprocessing...")
    feature_names = booster.feature_names
    X = preprocess_for_scoring(df, feature_names)

    print("üîÆ Scoring predictions...")
    probabilities = model.predict_proba(X)[:, 1]

    df['predicted_probability'] = probabilities
    df['username'] = username
    df['scored_at'] = datetime.now()
    df['model_name'] = 'xgb_model'
    df['rank'] = df['predicted_probability'].rank(method='first', ascending=False).astype(int)

    df['cosine_similarity'] = cosine_similarity_batch(
        np.stack(df['user_embedding']),
        np.stack(df['media_embedding'])
    )

    df['explanation'] = np.where(
        df['cosine_similarity'] > 0.85,
        "Very similar to your viewing preferences",
        ""
    )

    output = df[['username', 'rating_key', 'predicted_probability', 'model_name', 'scored_at', 'rank', 'cosine_similarity', 'explanation']]
    engine = get_engine()
    output.to_sql("recommendations", engine, if_exists="append", index=False)
    print(f"‚úÖ Stored {len(output)} recommendations for {username}.")

    top_df = df.nlargest(5, 'predicted_probability')
    explainer = shap.TreeExplainer(model)
    top_indices = top_df.index
    X_top = X[top_indices]
    shap_values = explainer.shap_values(X_top)

    print("\nüîç SHAP Feature Impact (Top 5 Rows):")
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    for i, row in enumerate(top_df.itertuples()):
        shap_row = shap_values[i]
        top_features = sorted(zip(feature_names, shap_row), key=lambda x: abs(x[1]), reverse=True)[:3]
        formatted = ", ".join([f"{name} ({value:+.2f})" for name, value in top_features])
        print(f"Rank {row.rank}: {row.rating_key} ‚Äî {formatted}")

        for dim, shap_val in enumerate(shap_row[:1536]):
            cur.execute(
            """
            INSERT INTO shap_impact (user_id, rating_key, dimension, shap_value)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (user_id, rating_key, dimension)
            DO UPDATE SET shap_value = EXCLUDED.shap_value
            """,
            (username, row.rating_key, dim, float(shap_val))
        )
    conn.commit()
    cur.close()
    conn.close()

def get_all_users():
    engine = get_engine()
    query = "SELECT DISTINCT username FROM watch_history"
    df = pd.read_sql(query, engine)
    return df['username'].tolist()

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--user", type=str, help="Username to score recommendations for")
    group.add_argument("--all-users", action="store_true", help="Score recommendations for all users with watch history")
    args = parser.parse_args()

    if args.all_users:
        engine = get_engine()
        with engine.begin() as conn:
            conn.execute(text("TRUNCATE recommendations"))
        users = get_all_users()
        print(f"üîÅ Scoring for all users: {users}")
        for user in users:
            score_and_store(user)
    else:
        score_and_store(args.user)
